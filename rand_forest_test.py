# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13SMjp03QxQsLrOreB87S0s9YKhEzqqb8
"""

!pip install kaggle
!pip install numpy==1.15.0
#Machine learning
!pip install catboost

import pandas as pd
#math operations
import numpy as np
#machine learning
from catboost import CatBoostRegressor, Pool
#data scaling
from sklearn.preprocessing import StandardScaler
#hyperparameter optimization
from sklearn.model_selection import GridSearchCV
#support vector machine model
from sklearn.svm import NuSVR, SVR
#kernel ridge model
from sklearn.kernel_ridge import KernelRidge
#data visualization
import matplotlib.pyplot as plt

# Colab's file access feature
from google.colab import files

#retrieve uploaded file
uploaded = files.upload()

#print results
for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))
  
# Then move kaggle.json into the folder where the API expects to find it.
!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json

!kaggle competitions list

!kaggle competitions download -c LANL-Earthquake-Prediction

!ls
!unzip train.csv.zip
!ls

import numpy as np
train_df = pd.read_csv('train.csv', nrows=6000000, iterator=True, chunksize=150_000, dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})

train_ad_sample_df = []
train_ttf_sample_df = []

for tdf in train_df:
    #print(tdf)
    train_ad_sample_df.append(tdf['acoustic_data'].values[::100])
    train_ttf_sample_df.append(tdf['time_to_failure'].values[::100])
#print('The shape of our features is:', train_df.shape)

def gen_features(X):
    strain = []
    strain.append(X.mean())
    strain.append(X.std())
    strain.append(X.min())
    strain.append(X.max())
    strain.append(X.kurtosis())
    strain.append(X.skew())
    #strain.append(np.quantile(X,0.01))
    #strain.append(pd.quantile(X,0.25))
    #strain.append(np.quantile(X,0.75))
    #strain.append(np.quantile(X,0.50))
    strain.append(np.abs(X).max())
    strain.append(np.abs(X).mean())
    strain.append(np.abs(X).std())
    return pd.Series(strain)

import pandas as pd
X_train = pd.DataFrame()
y_train = pd.Series()
for df in train_df:
    ch = gen_features(df['acoustic_data'])
    X_train = X_train.append(ch, ignore_index=True)
    y_train = y_train.append(pd.Series(df['time_to_failure'].values[-1]))

X_train, X_test, y_train, y_test = train_test_split(train_ad_sample_df, train_ttf_sample_df, test_size=0.2, random_state=0)

sc = StandardScaler()  
X_train = sc.fit_transform(X_train)  
X_test = sc.transform(X_test)

regressor = RandomForestRegressor(n_estimators= 1000, random_state=0)  
regressor.fit(X_train, y_train)

from sklearn import metrics
y_pred = regressor.predict(X_test) 
print(y_pred)
#print("\nX_train:\n")
#print(X_train.head())
#print(X_train.shape)

#print("\nX_test:\n")
#print(X_test.head())
#print(X_test.shape)

print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

train_ad_sample_df = train_df['acoustic_data'].values[::100]
train_ttf_sample_df = train_df['time_to_failure'].values[::100]
max_value = max(train_ttf_sample_df)
min_value = min(train_ttf_sample_df)
print(int((max_value - min_value)))

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

train_ad_sample_df = train_df['acoustic_data']
train_ttf_sample_df = train_df['time_to_failure'].values[::100]
max_value = max(train_ad_sample_df)
min_value = min(train_ad_sample_df)
bins = np.linspace(min_value, max_value, 15)


plt.hist(train_ad_sample_df, color = 'green', bins = bins)
plt.xlabel("acoustic_data")
plt.title('Histogram 1')
# plt.grid(axis='y', alpha=0.75)
plt.show()

train_ad_sample_df = train_df['acoustic_data'].values[::100]
# train_ttf_sample_df = train_df['time_to_failure'].values[::100]
max_value = max(train_ad_sample_df)
min_value = min(train_ad_sample_df)
bins = np.linspace(min_value, max_value, 30)

train_hist = [i for i in train_ad_sample_df if (i >= min_value and i <= -30) or i >= 40]
                                                                               
plt.hist(train_hist, color = 'green', bins = bins)
  
plt.xlabel("acoustic_data")
plt.title('Histogram 1')
plt.grid(axis='y', alpha=0.75)
plt.show()

train_df = pd.read_csv('train.csv', nrows=6000000,iterator=True, chunksize=150_000,dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})

def gen_features(X):
    strain = []
    strain.append(X.mean())
    strain.append(X.std())
    strain.append(X.min())
    strain.append(X.max())
    strain.append(X.kurtosis())
    strain.append(X.skew())
    #strain.append(np.quantile(X,0.01))
    #strain.append(pd.quantile(X,0.25))
    #strain.append(np.quantile(X,0.75))
    #strain.append(np.quantile(X,0.50))
    strain.append(np.abs(X).max())
    strain.append(np.abs(X).mean())
    strain.append(np.abs(X).std())
    return pd.Series(strain)

!ls
!unzip test.zip
!ls

train_df = pd.read_csv('train.csv', nrows=6000000, iterator=True, chunksize=150_000, dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})

X_train = pd.DataFrame()
y_train = pd.Series()
for df in train_df:
    ch = gen_features(df['acoustic_data'])
    X_train = X_train.append(ch, ignore_index=True)
    y_train = y_train.append(pd.Series(df['time_to_failure'].values[-1]))

train_ad_sample_df = []
train_ttf_sample_df = []

for tdf in train_df:
    #print(tdf)
    train_ad_sample_df.append(tdf['acoustic_data'].values[::100])
    train_ttf_sample_df.append(tdf['time_to_failure'].values[::100])
#print('The shape of our features is:', train_df.shape)

#data preprocessing
import pandas as pd
#math operations
import numpy as np
#machine learning
#from catboost import CatBoostRegressor, Pool
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import export_graphviz
#import pydot
#data scaling
from sklearn.preprocessing import StandardScaler
#hyperparameter optimization
from sklearn.model_selection import GridSearchCV
#support vector machine model
from sklearn.svm import NuSVR, SVR
#kernel ridge model
from sklearn.kernel_ridge import KernelRidge
#data visualization
import matplotlib.pyplot as plt

test_features = pd.read_csv('seg_00030f.csv')
    
X_test = gen_features(test_features['acoustic_data'])

# Instantiate model with 1000 decision trees
rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)
# Train the model on training data
rf.fit(X_train, y_train);

y_pred1 = rf.predict([X_test])
errors = abs(y_pred1 - y_train)
#print('Mean Absolute Error:', round(np.mean(errors), 4), 'degrees.')
print(y_pred1)

from sklearn.datasets import make_classification
from sklearn.ensemble import ExtraTreesClassifier

# Build a classification task using 3 informative features
X_train, y_train = make_classification(n_samples=1000,
                           n_features=10,
                           n_informative=3,
                           n_redundant=0,
                           n_repeated=0,
                           n_classes=2,
                           random_state=0,
                           shuffle=False)

# Build a forest and compute the feature importances
forest = ExtraTreesClassifier(n_estimators=250,
                              random_state=0)

forest.fit(X_train, y_train)

# Build a forest and compute the feature importances
forest = ExtraTreesClassifier(n_estimators=250,
                              random_state=0)

importances = forest.feature_importances_
std = np.std([tree.feature_importances_ for tree in forest.estimators_],
             axis=0)
indices = np.argsort(importances)[::-1]

# Print the feature ranking
print("Feature ranking:")

for f in range(X_train.shape[1]):
    print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))

# Plot the feature importances of the forest
plt.figure()
plt.title("Feature importances")
plt.bar(range(X_train.shape[1]), importances[indices],
       color="r", yerr=std[indices], align="center")
plt.xticks(range(X_train.shape[1]), indices)
plt.xlim([-1, X_train.shape[1]])
plt.show()