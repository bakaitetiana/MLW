# -*- coding: utf-8 -*-
"""Earthquake.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1apDwFgZJMkfUrOLR5x2qrwpQdIY-bI9p
"""

#SETUP part
!pip install -U -q kaggle
!mkdir -p ~/.kaggle
from google.colab import files
files.upload() # UPLOAD KAGGLE.JSON
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle competitions download -c LANL-Earthquake-Prediction
import zipfile
zipfile.ZipFile('/content/train.csv.zip', 'r').extractall('/content/')
zipfile.ZipFile('/content/test.zip', 'r').extractall('/content/test/')
!rm '/content/train.csv.zip'
!rm '/content/test.zip'
!pip install catboost

# IMPORTS
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from catboost import CatBoostRegressor, Pool
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score 
from sklearn import preprocessing
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.linear_model import ElasticNet
from sklearn.datasets import make_regression
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso

def gen_features(X):
    strain = []
    strain.append(X.mean())
    strain.append(X.std())
    strain.append(X.min())
    strain.append(X.max())
    strain.append(X.kurtosis())
    strain.append(X.skew())
    strain.append(np.quantile(X,0.01))
    strain.append(np.quantile(X,0.05))
    strain.append(np.quantile(X,0.95))
    strain.append(np.quantile(X,0.99))
    strain.append(np.abs(X).max())
    strain.append(np.abs(X).mean())
    strain.append(np.abs(X).std())
    return pd.Series(strain)


train = pd.read_csv('train.csv', iterator=True, chunksize=150_000, dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})
X_train = pd.DataFrame()
y_train = pd.Series()
for df in train:
    ch = gen_features(df['acoustic_data'])
    X_train = X_train.append(ch, ignore_index=True)
    y_train = y_train.append(pd.Series(df['time_to_failure'].values[-1]))
    
X_train, x_test, y_train, y_test = train_test_split(X_train,y_train, test_size = 0.2)

# CATBOOST
train_pool = Pool(X_train, y_train)
catreg = CatBoostRegressor(iterations=10000, loss_function='MAE', boosting_type='Ordered')
catreg.fit(X_train, y_train, silent=True)
preds = catreg.predict(x_test)
r2 = r2_score(y_test,preds)
print(r2)
print(catreg.get_best_score())

# LOGISTIC REGRESSION
logistic = LogisticRegression()
lab_enc = preprocessing.LabelEncoder()
y_encoded = lab_enc.fit_transform(y_train)
logistic.fit(X_train, y_encoded)
y_encoded = lab_enc.fit_transform(y_test)
predicted_classes = logistic.predict(x_test)
r2 = r2_score(y_encoded,predicted_classes)
print(r2)
# If the R2 score is negative than prediction makes no sense because if it is negative than prediction is even worse than a straight horizontal line

# POLYNOMIAL REGRESSION
poly = PolynomialFeatures(degree=4)
X = poly.fit_transform(X_train)
polyreg = LinearRegression()
lab_enc = preprocessing.LabelEncoder()
y_encoded = lab_enc.fit_transform(y_train)
polyreg.fit(X, y_encoded)
T = poly.fit_transform(x_test)
y_pred = polyreg.predict(T)
y_encoded = lab_enc.fit_transform(y_test)
r2 = r2_score(y_encoded,y_pred)
print(r2)
# If the R2 score is negative than prediction makes no sense because if it is negative than prediction is even worse than a straight horizontal line

# ELASTIC NET
X_train, y_train = make_regression(n_features=13, random_state=0)
regr = ElasticNet(random_state=0)
lab_enc = preprocessing.LabelEncoder()
y_encoded = lab_enc.fit_transform(y_train)
regr.fit(X_train, y_encoded)
y_pred = regr.predict(x_test)
y_encoded = lab_enc.fit_transform(y_test)
r2 = r2_score(y_encoded,y_pred)
print(r2)
# If the R2 score is negative than prediction makes no sense because if it is negative than prediction is even worse than a straight horizontal line

# RIDGE
rid = Ridge(alpha=100)
lab_enc = preprocessing.LabelEncoder()
y_encoded = lab_enc.fit_transform(y_train)
rid.fit(X_train, y_encoded)
y_pred = rid.predict(x_test)
y_encoded = lab_enc.fit_transform(y_test)
r2 = r2_score(y_encoded,y_pred)
print(r2)
# If the R2 score is negative than prediction makes no sense because if it is negative than prediction is even worse than a straight horizontal line

# LASSO
lasso = Lasso()
lab_enc = preprocessing.LabelEncoder()
y_encoded = lab_enc.fit_transform(y_train)
lasso.fit(X_train,y_encoded)
y_pred = lasso.predict(x_test)
y_encoded = lab_enc.fit_transform(y_test)
r2 = r2_score(y_encoded,y_pred)
print(r2)
# If the R2 score is negative than prediction makes no sense because if it is negative than prediction is even worse than a straight horizontal line